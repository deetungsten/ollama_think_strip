# /etc/nginx/conf.d/ollama_proxy.conf
# ------------------------------------------------------------
#  Home-Assistant (Ollama integration) → NGINX+Lua → llama-cpp
# ------------------------------------------------------------

lua_package_path '/usr/local/openresty/lualib/?.lua;;';
lua_need_request_body on;
client_body_buffer_size 16k;

# Uncomment these lines if you want to log API requests/responses
# log_format api_logger escape=none '{"request_body":$request_body,"response_body":$resp_body}';
# access_log /var/log/nginx/ollama_requests.log api_logger;

# Define upstream server for llama-cpp
upstream llama_cpp_upstream {
    server llama-cpp:8000;  # container name or IP:port
}

server {
    listen 11434;  # Standard Ollama port that Home Assistant expects

    # ------------ /api/chat → /v1/chat/completions ------------
    location /api/chat {
        content_by_lua_block {
            local http = require "resty.http"
            local json = require "cjson.safe"
            local gsub = ngx.re.gsub

            -- Read the request body
            ngx.req.read_body()
            local body_data = ngx.req.get_body_data()
            
            if not body_data then
                ngx.status = 400
                ngx.say(json.encode{error = "Empty request body"})
                return
            end

            local req, err = json.decode(body_data)
            if not req then
                ngx.status = 400
                ngx.say(json.encode{error = "Invalid JSON: " .. (err or "unknown error")})
                return
            end

            -- Transform Ollama request to llama-cpp format
            local llama_req = {
                model = req.model,
                messages = req.messages,
                stream = req.stream or false
            }

            -- Handle optional parameters
            if req.options then
                if req.options.num_predict then
                    llama_req.max_tokens = req.options.num_predict
                end
                if req.options.temperature then
                    llama_req.temperature = req.options.temperature
                end
                if req.options.top_p then
                    llama_req.top_p = req.options.top_p
                end
                if req.options.top_k then
                    llama_req.top_k = req.options.top_k
                end
            end

            -- Make request to llama-cpp server
            local httpc = http.new()
            local res, err = httpc:request_uri(
                "http://llama-cpp:8000/v1/chat/completions", {
                    method = "POST",
                    body = json.encode(llama_req),
                    headers = {["Content-Type"] = "application/json"},
                    keepalive = true,
                    timeout = 600000  -- 10 minutes timeout
                }
            )

            if not res then
                ngx.status = 502
                ngx.say(json.encode{error = "llama-cpp unreachable: " .. (err or "unknown error")})
                return
            end

            -- Function to strip <think> tags if present
            local function strip_think(t)
                if not t then return "" end
                return gsub(t, [[<think\b[^>]*>[\s\S]*?</think>]], "", "ijo")
            end

            -- Set appropriate content type based on streaming or not
            ngx.header["Content-Type"] = req.stream and "application/x-ndjson" or "application/json"

            -- Handle streaming response
            if req.stream then
                for line in res.body:gmatch("[^\r\n]+") do
                    if #line > 0 then
                        -- Skip SSE prefix if present
                        local json_line = line
                        if line:sub(1, 6) == "data: " then
                            json_line = line:sub(7)
                        end
                        
                        -- Skip empty data markers
                        if json_line == "[DONE]" then
                            -- End of stream marker
                            ngx.flush(true)
                        else
                            local ok, oai = pcall(json.decode, json_line)
                            if ok and oai and oai.choices and oai.choices[1] then
                                local delta = oai.choices[1].delta or {}
                                local finish = oai.choices[1].finish_reason ~= nil
                                
                                -- Format as Ollama streaming response
                                ngx.print(json.encode{
                                    model = req.model,
                                    created_at = os.date("!%Y-%m-%dT%H:%M:%SZ"),
                                    message = {
                                        role = delta.role or "assistant",
                                        content = strip_think(delta.content or "")
                                    },
                                    done = finish
                                } .. "\n")
                                ngx.flush(true)
                            end
                        end
                    end
                end
            else
                -- Handle non-streaming response
                local ok, oai = pcall(json.decode, res.body)
                if not ok or not oai or not oai.choices or not oai.choices[1] or not oai.choices[1].message then
                    ngx.status = 502
                    ngx.say(json.encode{error = "Invalid response from llama-cpp"})
                    return
                end
                
                local msg = oai.choices[1].message
                
                -- Format as Ollama non-streaming response
                ngx.say(json.encode{
                    model = req.model,
                    created_at = os.date("!%Y-%m-%dT%H:%M:%SZ"),
                    message = {
                        role = msg.role or "assistant",
                        content = strip_think(msg.content or "")
                    },
                    done = true
                })
            end
        }
    }

    # ------------ /api/generate → /v1/completions -----------
    location /api/generate {
        content_by_lua_block {
            local http = require "resty.http"
            local json = require "cjson.safe"

            -- Read the request body
            ngx.req.read_body()
            local body_data = ngx.req.get_body_data()
            
            if not body_data then
                ngx.status = 400
                ngx.say(json.encode{error = "Empty request body"})
                return
            end

            local req, err = json.decode(body_data)
            if not req then
                ngx.status = 400
                ngx.say(json.encode{error = "Invalid JSON: " .. (err or "unknown error")})
                return
            end

            -- Transform Ollama generate request to llama-cpp completions format
            local llama_req = {
                model = req.model,
                prompt = req.prompt,
                stream = req.stream or false
            }

            -- Handle optional parameters
            if req.options then
                if req.options.num_predict then
                    llama_req.max_tokens = req.options.num_predict
                end
                if req.options.temperature then
                    llama_req.temperature = req.options.temperature
                end
                if req.options.top_p then
                    llama_req.top_p = req.options.top_p
                end
                if req.options.top_k then
                    llama_req.top_k = req.options.top_k
                end
            end

            -- Make request to llama-cpp server
            local httpc = http.new()
            local res, err = httpc:request_uri(
                "http://llama-cpp:8000/v1/completions", {
                    method = "POST",
                    body = json.encode(llama_req),
                    headers = {["Content-Type"] = "application/json"},
                    keepalive = true,
                    timeout = 600000  -- 10 minutes timeout
                }
            )

            if not res then
                ngx.status = 502
                ngx.say(json.encode{error = "llama-cpp unreachable: " .. (err or "unknown error")})
                return
            end

            -- Set appropriate content type based on streaming or not
            ngx.header["Content-Type"] = req.stream and "application/x-ndjson" or "application/json"

            -- Handle streaming response
            if req.stream then
                for line in res.body:gmatch("[^\r\n]+") do
                    if #line > 0 then
                        -- Skip SSE prefix if present
                        local json_line = line
                        if line:sub(1, 6) == "data: " then
                            json_line = line:sub(7)
                        end
                        
                        -- Skip empty data markers
                        if json_line == "[DONE]" then
                            -- End of stream marker
                            ngx.flush(true)
                        else
                            local ok, oai = pcall(json.decode, json_line)
                            if ok and oai and oai.choices and oai.choices[1] then
                                local text = oai.choices[1].text or ""
                                local finish = oai.choices[1].finish_reason ~= nil
                                
                                -- Format as Ollama streaming response
                                ngx.print(json.encode{
                                    model = req.model,
                                    created_at = os.date("!%Y-%m-%dT%H:%M:%SZ"),
                                    response = text,
                                    done = finish
                                } .. "\n")
                                ngx.flush(true)
                            end
                        end
                    end
                end
            else
                -- Handle non-streaming response
                local ok, oai = pcall(json.decode, res.body)
                if not ok or not oai or not oai.choices or not oai.choices[1] then
                    ngx.status = 502
                    ngx.say(json.encode{error = "Invalid response from llama-cpp"})
                    return
                end
                
                -- Format as Ollama non-streaming response
                ngx.say(json.encode{
                    model = req.model,
                    created_at = os.date("!%Y-%m-%dT%H:%M:%SZ"),
                    response = oai.choices[1].text or "",
                    done = true
                })
            end
        }
    }

    # ------------- /api/tags → /v1/models ----------------------
    location /api/tags {
        content_by_lua_block {
            local http = require "resty.http"
            local json = require "cjson.safe"

            -- Request models from llama-cpp
            local httpc = http.new()
            local res, err = httpc:request_uri(
                "http://llama-cpp:8000/v1/models", {
                    method = "GET"
                }
            )

            if not res then
                ngx.status = 502
                ngx.say(json.encode{error = "llama-cpp unreachable: " .. (err or "unknown error")})
                return
            end

            local ok, list = pcall(json.decode, res.body)
            if not ok or not list then
                ngx.status = 502
                ngx.say(json.encode{error = "Invalid response from llama-cpp"})
                return
            end

            -- Transform llama-cpp models list to Ollama format
            local models = {}
            for _, m in ipairs(list.data or {}) do
                table.insert(models, {
                    name = m.id,
                    size = "",  -- llama-cpp doesn't provide size info
                    modified_at = os.date("!%Y-%m-%dT%H:%M:%SZ"),
                    digest = m.id  -- Using ID as digest as llama-cpp doesn't have this concept
                })
            end

            ngx.header["Content-Type"] = "application/json"
            ngx.say(json.encode{models = models})
        }
    }

    # Optional endpoint for health check
    location /api/health {
        content_by_lua_block {
            local http = require "resty.http"
            local json = require "cjson.safe"
            
            -- Check if llama-cpp is reachable
            local httpc = http.new()
            local res, err = httpc:request_uri(
                "http://llama-cpp:8000/v1/models", {
                    method = "GET",
                    timeout = 5000  -- 5 second timeout for health check
                }
            )

            if not res then
                ngx.status = 503
                ngx.say(json.encode{status = "error", message = "llama-cpp unreachable"})
                return
            end

            ngx.status = 200
            ngx.say(json.encode{status = "ok"})
        }
    }

    # Default handler for unmapped endpoints
    location / {
        return 404;
    }
}