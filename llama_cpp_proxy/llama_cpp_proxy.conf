# /etc/nginx/conf.d/ollama_proxy.conf
# ------------------------------------------------------------
#  Home-Assistant (Ollama integration) → Lua shim → llama-cpp
# ------------------------------------------------------------

# -------- logging identical to your previous setup ----------
# log_format api_logger escape=none '{"request_body":$request_body,"response_body":$resp_body}';
# access_log  /var/log/nginx/ollama_requests.log  api_logger;

lua_package_path '/usr/local/openresty/lualib/?.lua;/usr/local/lib/lua/5.1/?.lua;/usr/local/share/lua/5.1/?.lua;;';
lua_need_request_body  on;
client_body_buffer_size 16k;

resolver 127.0.0.11 ipv6=off;  # Docker DNS resolver

# ---------- single upstream for clarity ---------------------
upstream llama_cpp_upstream {
    server llama-cpp:8000;
}

server {
    listen 11434;                  # the port Home Assistant expects

    # Add proxy timeouts
    proxy_connect_timeout 600s;
    proxy_send_timeout 600s;
    proxy_read_timeout 600s;

    # ------------ /api/chat → /v1/chat/completions ------------
    location /api/chat {
        content_by_lua_block {
            local http  = require "resty.http"
            local json  = require "cjson.safe"
            local gsub  = ngx.re.gsub

            ngx.req.read_body()
            local req = json.decode(ngx.req.get_body_data() or "{}") or {}

            local llama_req = {
                model      = req.model,
                messages   = req.messages,
                stream     = req.stream or false,
                max_tokens = req.options and req.options.num_predict
            }

            -- the Lua side cannot use an NGINX "upstream" name,
            -- so call the host directly:
            local httpc = http.new()
            httpc:set_timeout(600000)  -- 10 minutes timeout
            local res, err = httpc:request_uri(
                 "http://llama-cpp:8000/v1/chat/completions", {
                     method  = "POST",
                     body    = json.encode(llama_req),
                     headers = {["Content-Type"]="application/json"},
                     keepalive = true,
                     timeout   = 600000
                 })
            if not res then
                ngx.status = 502
                ngx.say(json.encode{error = "llama-cpp unreachable: "..err})
                return
            end

            local function strip_think(t)
                return gsub(t or "", [[<think\b[^>]*>[\s\S]*?</think>]], "", "ijo")
            end

            ngx.header["Content-Type"] =
                (req.stream and "application/x-ndjson") or "application/json"

            if req.stream then
                for line in res.body:gmatch("[^\r\n]+") do
                    if #line > 0 and line:sub(1,6) ~= "data: " then
                        local ok, oai = pcall(json.decode, line)
                        if ok then
                            local delta  = oai.choices[1].delta or {}
                            local finish = oai.choices[1].finish_reason ~= nil
                            ngx.print(json.encode{
                                model      = req.model,
                                created_at = os.date("!%Y-%m-%dT%H:%M:%SZ"),
                                message    = {role="assistant",
                                              content = strip_think(delta.content)},
                                done       = finish
                            }.."\n")
                            ngx.flush(true)
                        end
                    end
                end
            else
                local oai = json.decode(res.body)
                local msg = oai.choices[1].message
                ngx.say(json.encode{
                    model      = req.model,
                    created_at = os.date("!%Y-%m-%dT%H:%M:%SZ"),
                    message    = {role = msg.role or "assistant",
                                  content = strip_think(msg.content)},
                    done       = true
                })
            end
        }
    }

    # -------- /api/generate → /v1/completions (optional) -------
    location /api/generate {
        proxy_pass       http://llama_cpp_upstream/v1/completions;
        proxy_set_header Host $host;
        proxy_set_header Content-Type "application/json";
    }

    # ------------- /api/tags → /v1/models ----------------------
    location /api/tags {
        content_by_lua_block {
            local http = require "resty.http"
            local json = require "cjson.safe"

            local res, err = http.new():request_uri(
                 "http://llama-cpp:8000/v1/models", { method = "GET" })
            if not res then
                ngx.status = 502
                ngx.say(json.encode{error = "llama-cpp unreachable: "..err})
                return
            end

            local list   = json.decode(res.body) or {}
            local models = {}
            for _, m in ipairs(list.data or {}) do
                models[#models+1] = {
                    model       = m.id,
                    name       = m.id,
                    size       = "0 B",
                    modified_at = os.date("!%Y-%m-%dT%H:%M:%SZ")
                }
            end
            -- If no models found, at least return the one from docker-compose
            if #models == 0 then
                models[1] = {
                    model      = "your-model",
                    name       = "your-model",
                    size       = "0 B",
                    modified_at = os.date("!%Y-%m-%dT%H:%M:%SZ")
                }
            end
            ngx.header["Content-Type"] = "application/json"
            ngx.say(json.encode{models = models})
        }
    }

    location / { return 404; }
}