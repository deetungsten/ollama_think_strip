version: '3'

services:
  nginx:
    build: .
    ports:
      - "11434:11434"
    volumes:
      - ./llama_cpp_proxy.conf:/etc/nginx/conf.d/default.conf:ro
      - ./logs:/var/log/nginx
    depends_on:
      - llamacpp
    networks:
      - llama_network

  llamacpp:
    image: ghcr.io/ggerganov/llama.cpp:server
    # Adjust these parameters based on your model requirements
    command: >
      --model /models/your-model.gguf
      --host 0.0.0.0
      --port 8000
      --cont-batching
    volumes:
      - ./models:/models
    networks:
      - llama_network

networks:
  llama_network:
    driver: bridge