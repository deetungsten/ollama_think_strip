version: '3'

services:
  nginx:
    image: openresty/openresty:jammy
    ports:
      - "11434:11434"
    volumes:
      - ~/nginx/llama_cpp_proxy.conf:/etc/nginx/conf.d/default.conf
      - ~/nginx/logs:/var/log/nginx
    depends_on:
      - llamacpp
    networks:
      - llama_network

  llamacpp:
    image: ghcr.io/ggerganov/llama.cpp:server
    # Adjust these parameters based on your model requirements
    command: >
      --model /models/your-model.gguf
      --host 0.0.0.0
      --port 8080
      --cont-batching
    volumes:
      - ~/models:/models
    networks:
      - llama_network

networks:
  llama_network:
    driver: bridge